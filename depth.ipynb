{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data import *\n",
    "from model import PTModel\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _is_pil_image(img):\n",
    "    return isinstance(img, Image.Image)\n",
    "\n",
    "def _is_numpy_image(img):\n",
    "    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __call__(self, sample):\n",
    "        image, depth = sample['image'], sample['depth']\n",
    "\n",
    "        if not _is_pil_image(image):\n",
    "            raise TypeError(\n",
    "                'img should be PIL Image. Got {}'.format(type(image)))\n",
    "        if not _is_pil_image(depth):\n",
    "            raise TypeError(\n",
    "                'img should be PIL Image. Got {}'.format(type(depth)))\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            depth = depth.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        return {'image': image, 'depth': depth}\n",
    "\n",
    "class RandomChannelSwap(object):\n",
    "    def __init__(self, probability):\n",
    "        from itertools import permutations\n",
    "        self.probability = probability\n",
    "        self.indices = list(permutations(range(3), 3))\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, depth = sample['image'], sample['depth']\n",
    "        if not _is_pil_image(image): raise TypeError('img should be PIL Image. Got {}'.format(type(image)))\n",
    "        if not _is_pil_image(depth): raise TypeError('img should be PIL Image. Got {}'.format(type(depth)))\n",
    "        if random.random() < self.probability:\n",
    "            image = np.asarray(image)\n",
    "            image = Image.fromarray(image[...,list(self.indices[random.randint(0, len(self.indices) - 1)])])\n",
    "        return {'image': image, 'depth': depth}\n",
    "\n",
    "def loadZipToMem(zip_file):\n",
    "    # Load zip file into memory\n",
    "    print('Loading dataset zip file...', end='')\n",
    "    from zipfile import ZipFile\n",
    "    input_zip = ZipFile(zip_file)\n",
    "    data = {name: input_zip.read(name) for name in input_zip.namelist()}\n",
    "    nyu2_train = list((row.split(',') for row in (data['data/nyu2_train.csv']).decode(\"utf-8\").split('\\n') if len(row) > 0))\n",
    "\n",
    "    from sklearn.utils import shuffle\n",
    "    nyu2_train = shuffle(nyu2_train, random_state=0)\n",
    "\n",
    "    #if True: nyu2_train = nyu2_train[:40]\n",
    "\n",
    "    print('Loaded ({0}).'.format(len(nyu2_train)))\n",
    "    return data, nyu2_train\n",
    "\n",
    "class depthDatasetMemory(Dataset):\n",
    "    def __init__(self, data, nyu2_train, transform=None):\n",
    "        self.data, self.nyu_dataset = data, nyu2_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.nyu_dataset[idx]\n",
    "        image = Image.open( BytesIO(self.data[sample[0]]) )\n",
    "        depth = Image.open( BytesIO(self.data[sample[1]]) )\n",
    "        sample = {'image': image, 'depth': depth}\n",
    "        if self.transform: sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.nyu_dataset)\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self,is_test=False):\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, depth = sample['image'], sample['depth']\n",
    "        \n",
    "        # image = self.to_tensor(image)\n",
    "\n",
    "        depth = depth.resize((320, 240))\n",
    "        print(np.array(depth))\n",
    "        if self.is_test:\n",
    "            depth = self.to_tensor(depth).float() / 1000\n",
    "        else:            \n",
    "            depth = self.to_tensor(depth).float() * 10000\n",
    "        print(depth)\n",
    "        # put in expected range\n",
    "        depth = torch.clamp(depth, 10, 1000)\n",
    "        # print(depth)\n",
    "        return {'image': image, 'depth': depth}\n",
    "\n",
    "    def to_tensor(self, pic):\n",
    "        if not(_is_pil_image(pic) or _is_numpy_image(pic)):\n",
    "            raise TypeError(\n",
    "                'pic should be PIL Image or ndarray. Got {}'.format(type(pic)))\n",
    "\n",
    "        if isinstance(pic, np.ndarray):\n",
    "            print('Im here at isinstance')\n",
    "            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n",
    "\n",
    "            return img.float().div(255)\n",
    "\n",
    "        # handle PIL Image\n",
    "        if pic.mode == 'I':\n",
    "            print('pic mode I')\n",
    "            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
    "        elif pic.mode == 'I;16':\n",
    "            print('pic mode I;16')\n",
    "            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n",
    "        else:\n",
    "            print('pic mode else')\n",
    "            img = torch.ByteTensor(\n",
    "                torch.ByteStorage.from_buffer(pic.tobytes()))\n",
    "        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n",
    "        if pic.mode == 'YCbCr':\n",
    "            nchannel = 3\n",
    "        elif pic.mode == 'I;16':\n",
    "            nchannel = 1\n",
    "        else:\n",
    "            nchannel = len(pic.mode)\n",
    "        img = img.view(pic.size[1], pic.size[0], nchannel)\n",
    "\n",
    "        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
    "        if isinstance(img, torch.ByteTensor):\n",
    "            print('divided')\n",
    "            return img.float().div(255)\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "def getNoTransform(is_test=False):\n",
    "    return transforms.Compose([\n",
    "        ToTensor(is_test=is_test)\n",
    "    ])\n",
    "\n",
    "def getDefaultTrainTransform():\n",
    "    return transforms.Compose([\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomChannelSwap(0.5),\n",
    "        ToTensor()\n",
    "    ])\n",
    "\n",
    "class depthDataset(Dataset):\n",
    "    def __init__(self, transform=None, is_test=False):\n",
    "        self.data = []\n",
    "        # self.data = dataset\n",
    "        if not is_test: \n",
    "            with open('train.txt', 'r') as f:\n",
    "                self.data = f.readlines()\n",
    "                self.data = [line.rstrip() for line in self.data]\n",
    "        else:  \n",
    "            with open('test.txt', 'r') as f:\n",
    "                self.data = f.readlines()\n",
    "                self.data = [line.rstrip() for line in self.data]  \n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        # print(self.data[idx], type(self.data[idx]), len(self.data[idx]))\n",
    "        image = Image.open( self.data[idx].split('\\t')[0] )\n",
    "        depth = Image.open( self.data[idx].split('\\t')[1] )\n",
    "        # print(depth)\n",
    "        sample = {'image': image, 'depth': depth}\n",
    "        if self.transform: sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def getTrainingTestingData(batch_size):\n",
    "    data, nyu2_train = loadZipToMem('nyu_data.zip')\n",
    "\n",
    "    transformed_training = depthDatasetMemory(data, nyu2_train, transform=getDefaultTrainTransform())\n",
    "    transformed_testing = depthDatasetMemory(data, nyu2_train, transform=getNoTransform())\n",
    "\n",
    "    return DataLoader(transformed_training, batch_size, shuffle=True), DataLoader(transformed_testing, batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset zip file...Loaded (50688).\n"
     ]
    }
   ],
   "source": [
    "data, nyu = loadZipToMem('nyu_data.zip')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8027/1868780939.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformed_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdepthDatasetMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnyu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetDefaultTrainTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "transformed_training = depthDatasetMemory(data, nyu, transform=getDefaultTrainTransform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pic mode else\n",
      "divided\n",
      "[[24 24 24 ... 30 30 30]\n",
      " [24 24 24 ... 30 30 30]\n",
      " [24 24 24 ... 30 30 30]\n",
      " ...\n",
      " [27 27 27 ... 21 21 21]\n",
      " [27 27 27 ... 21 21 21]\n",
      " [27 27 27 ... 21 21 21]]\n",
      "pic mode else\n",
      "divided\n",
      "tensor([[[ 94.1176,  94.1176,  94.1176,  ..., 117.6471, 117.6471, 117.6471],\n",
      "         [ 94.1176,  94.1176,  94.1176,  ..., 117.6471, 117.6471, 117.6471],\n",
      "         [ 94.1176,  94.1176,  94.1176,  ..., 117.6471, 117.6471, 117.6471],\n",
      "         ...,\n",
      "         [105.8824, 105.8824, 105.8824,  ...,  82.3529,  82.3529,  82.3529],\n",
      "         [105.8824, 105.8824, 105.8824,  ...,  82.3529,  82.3529,  82.3529],\n",
      "         [105.8824, 105.8824, 105.8824,  ...,  82.3529,  82.3529,  82.3529]]])\n"
     ]
    }
   ],
   "source": [
    "# sample = transformed_training.nyu_dataset[0]\n",
    "# depth = Image.open( BytesIO(transformed_training.data[sample[1]]) )\n",
    "# np.array(depth)\n",
    "x = transformed_training[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/nyu2_train/bathroom_0053_out/66.jpg', 'data/nyu2_train/bathroom_0053_out/66.png']\n"
     ]
    }
   ],
   "source": [
    "print(nyu[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = '/media/krishna/Data/generated_2.1.0/train/*/*/raw_images/*'\n",
    "depth_images = '/media/krishna/Data/generated_2.1.0/train/*/*/depth_images/*'\n",
    "\n",
    "files = {'depth': [], 'rgb': []}\n",
    "for name in glob.glob(pathname=depth_images):\n",
    "    files['depth'].append(name)\n",
    "\n",
    "for name in glob.glob(pathname=rgb):\n",
    "    files['rgb'].append(name)\n",
    "\n",
    "print(len(files['depth']), len(files['rgb']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# idx = random.randint(0,333623)\n",
    "dataset = []\n",
    "for i in range(60000):\n",
    "    idx = random.randint(0,333623)\n",
    "    rgb, depth = files['rgb'][idx], files['depth'][idx]\n",
    "    dataset.append([rgb, depth])\n",
    "\n",
    "with open('train.txt', 'w') as f:\n",
    "    for i in range(50000):\n",
    "        f.write(\"%s\\t%s\\n\" % (dataset[i][0], dataset[i][1]))\n",
    "\n",
    "\n",
    "with open('test.txt', 'w') as f:\n",
    "    for i in range(50000, 60000):\n",
    "        f.write(\"%s\\t%s\\n\" % (dataset[i][0], dataset[i][1]))\n",
    "# print(rgb, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Unet_depth20.pth'\n",
    "model = PTModel()\n",
    "model.load_state_dict(torch.load(path))\n",
    "print('Model successfully loaded.')\n",
    "\n",
    "transformed_testing = depthDataset(transform=getNoTransform(), is_test=True)\n",
    "\n",
    "y = model(transformed_testing[0]['image'].unsqueeze(0))\n",
    "im = np.array(y[0][0].detach().cpu().numpy())\n",
    "# im = 1./im\n",
    "im = np.divide(5000,im)\n",
    "print(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8027/3958306161.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformed_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdepthDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetDefaultTrainTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_8027/115201502.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, transform, is_test)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# self.data = dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.txt'"
     ]
    }
   ],
   "source": [
    "transformed_training = depthDataset(transform=getDefaultTrainTransform(), is_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 300, 300])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_training[0]['depth'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.PngImagePlugin.PngImageFile image mode=L size=300x300 at 0x7F10563F7D90>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[28, 28, 28,  ..., 29, 29, 29],\n",
       "         [28, 28, 28,  ..., 29, 29, 29],\n",
       "         [28, 28, 28,  ..., 29, 29, 29],\n",
       "         ...,\n",
       "         [36, 36, 36,  ..., 36, 36, 36],\n",
       "         [36, 36, 36,  ..., 36, 36, 36],\n",
       "         [36, 36, 36,  ..., 36, 36, 36]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = T.Compose([T.PILToTensor()])\n",
    "transform(transformed_training[0]['depth'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "926530c59d0c67f6c886d6511c9f4bd40f6de4d0e828a14fb7978dda331b09f5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('et': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
